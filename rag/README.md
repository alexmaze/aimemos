# RAG æ¨¡å—ä½¿ç”¨æŒ‡å—

æœ¬ç›®å½•åŒ…å« AIMemos é¡¹ç›®çš„ RAG (Retrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆ) æ¨¡å—å®ç°ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
rag/
â”œâ”€â”€ DESIGN_RAG.md       # è¯¦ç»†è®¾è®¡æ–‡æ¡£ï¼ˆä¸­æ–‡ï¼‰
â”œâ”€â”€ requirements.txt    # Python ä¾èµ–
â”œâ”€â”€ embeddings.py       # m3e-base åµŒå…¥æ¨¡å‹å°è£…
â”œâ”€â”€ vector_store.py     # Milvus Lite å‘é‡æ•°æ®åº“æ“ä½œ
â”œâ”€â”€ ingest.py          # æ–‡æ¡£æ‘„å–ä¸ç´¢å¼•æ„å»º
â”œâ”€â”€ llm_client.py      # OpenAI å…¼å®¹çš„ LLM å®¢æˆ·ç«¯
â””â”€â”€ README.md          # æœ¬æ–‡ä»¶

pocketflow/
â””â”€â”€ rag_workflow.yaml  # PocketFlow å·¥ä½œæµå®šä¹‰
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… RAG æ¨¡å—ä¾èµ–
pip install -r rag/requirements.txt

# æˆ–è€…ä½¿ç”¨ uvï¼ˆå¦‚æœé¡¹ç›®å·²é›†æˆï¼‰
uv pip install -r rag/requirements.txt
```

### 2. å‡†å¤‡æ•°æ®

åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•å¹¶æ·»åŠ ä¸€äº›æ–‡æ¡£ï¼š

```bash
mkdir -p data/kb
echo "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚" > data/kb/ai_intro.txt
echo "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ã€‚" > data/kb/ml_intro.txt
echo "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚" > data/kb/dl_intro.txt
```

### 3. æµ‹è¯•åµŒå…¥æ¨¡å‹

é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ moka-ai/m3e-base æ¨¡å‹ï¼ˆçº¦ 400MBï¼‰ï¼š

```bash
cd rag
python embeddings.py
```

é¢„æœŸè¾“å‡ºï¼š
```
Loading tokenizer and model: moka-ai/m3e-base
Using device: cuda  # æˆ– cpu
Model loaded successfully. Embedding dimension: 768
...
```

### 4. å¯åŠ¨ Milvus Lite

Milvus Lite æ˜¯å†…åµŒå¼å‘é‡æ•°æ®åº“ï¼Œæ— éœ€å•ç‹¬å®‰è£…æœåŠ¡ã€‚é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨åˆ›å»ºæ•°æ®åº“æ–‡ä»¶ã€‚

### 5. ç´¢å¼•æ–‡æ¡£

è¿è¡Œæ–‡æ¡£æ‘„å–è„šæœ¬ï¼Œå°† `data/kb` ç›®å½•ä¸­çš„æ–‡æ¡£ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“ï¼š

```bash
cd rag
python ingest.py --data-dir ../data/kb --kb-id test_kb
```

å‚æ•°è¯´æ˜ï¼š
- `--data-dir`: æ–‡æ¡£ç›®å½•ï¼ˆé»˜è®¤ï¼š./data/kbï¼‰
- `--kb-id`: çŸ¥è¯†åº“ IDï¼ˆé»˜è®¤ï¼šdefaultï¼‰
- `--milvus-uri`: Milvus æ•°æ®åº“è·¯å¾„ï¼ˆé»˜è®¤ï¼š./milvus_demo.dbï¼‰
- `--max-tokens`: æ¯å—æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ï¼š512ï¼‰
- `--overlap-tokens`: å—é‡å  token æ•°ï¼ˆé»˜è®¤ï¼š128ï¼‰
- `--batch-size`: åµŒå…¥æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š32ï¼‰
- `--recreate-index`: é‡å»ºç´¢å¼•ï¼ˆåˆ é™¤ç°æœ‰æ•°æ®ï¼‰

é¢„æœŸè¾“å‡ºï¼š
```
=== RAG Document Ingestion Pipeline ===

Initializing embedder...
Loading tokenizer and model: moka-ai/m3e-base
...
Loading documents...
Loaded 3 documents

Step 1: Chunking documents...
Total chunks created: 5

Step 2: Generating embeddings...
Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, ...]
Generated 5 embeddings

Step 3: Inserting into vector store...
  Inserted batch 1: 5 chunks

=== Ingestion completed ===
Total chunks inserted: 5
```

### 6. å¯åŠ¨æœ¬åœ° LLMï¼ˆå¯é€‰ï¼‰

ä¸ºäº†æµ‹è¯•å®Œæ•´çš„ RAG æµç¨‹ï¼Œéœ€è¦ä¸€ä¸ªæä¾› OpenAI å…¼å®¹ API çš„æœ¬åœ° LLMã€‚

#### é€‰é¡¹ A: ä½¿ç”¨ vLLM

```bash
# å®‰è£… vLLM
pip install vllm

# å¯åŠ¨æœåŠ¡ï¼ˆç¤ºä¾‹ï¼šä½¿ç”¨ Qwen æ¨¡å‹ï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen-7B-Chat \
    --host 0.0.0.0 \
    --port 8000
```

#### é€‰é¡¹ B: ä½¿ç”¨ FastChat

```bash
# å®‰è£… FastChat
pip install fschat

# å¯åŠ¨ OpenAI å…¼å®¹æœåŠ¡
python -m fastchat.serve.openai_api_server \
    --host 0.0.0.0 \
    --port 8000
```

#### é€‰é¡¹ C: ä½¿ç”¨ Ollama

```bash
# å®‰è£… Ollama (https://ollama.ai)
# æ‹‰å–æ¨¡å‹
ollama pull qwen:7b

# Ollama é»˜è®¤åœ¨ http://localhost:11434 æä¾› API
# éœ€è¦è®¾ç½® base_url ä¸º http://localhost:11434/v1
```

### 7. æµ‹è¯• LLM å®¢æˆ·ç«¯

è®¾ç½®ç¯å¢ƒå˜é‡å¹¶æµ‹è¯•è¿æ¥ï¼š

```bash
export OPENAI_BASE_URL='http://localhost:8000/v1'
export OPENAI_API_KEY='EMPTY'  # å¦‚æœä¸éœ€è¦ API key

cd rag
python llm_client.py --prompt "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
```

é¢„æœŸè¾“å‡ºï¼š
```
=== LLM Client Test ===

Base URL: http://localhost:8000/v1
Testing connection...

Connection successful!

System: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹
User: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±